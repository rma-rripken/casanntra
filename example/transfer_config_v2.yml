# ===========================
# TRANSFER LEARNING CONFIGURATION FILE
# ===========================
# This file defines the training sequence for staged transfer learning.
# Each step builds upon prior models, allowing for progressive learning.
# The user can selectively re-execute steps as needed.
# ===========================

output_dir: ./output  # Directory for saving output files

model_builder_config:
  # Defines which ModelBuilder subclass is used.
  builder_name: MultiStageModelBuilder  # Uses a staged learning model
  args:
    input_names:  # Features (input variables)
      - northern_flow
      - exports
      - sjr_flow
      - cu_delta
      - sf_tidal_energy
      - sf_tidal_filter
      - dcc
      - smscg
    output_names:  # Output variables with their scaling factors
      x2: 100.0    # Normalized based on known physical ranges
      mrz: 30000.0
      pct: 20000.0
      mal: 15000.0  # 2
      gzl: 20000.0
      bdl: 12000.0  # 5
      nsl2: 12000.0
      cse: 12000.0
      emm2: 3000.0
      tms: 3000.0    # 9
      anh: 6000.0
      jer: 2500.0
      sal: 1200.0
      frk: 2500.0
      srv: 1200.0
      bac: 1500.0
      rsl: 1500.0
      oh4: 1200.0
      wci: 1200.0
      trp: 1200.0    # 18
    ndays: 90  # Defines the time window for GRU model

# ===========================
# TRAINING STEPS
# ===========================
# Each step corresponds to a training phase. Models are trained in stages.
# ===========================

steps:
- name: dsm2_base  # Step 1: Train DSM2 (1D Model) on its original dataset
  input_prefix: dsm2_base  # Points to DSM2 training data
  #input_mask_regex: None # [dsm2_base_9.\.csv, dsm2_base_100\.csv, dsm2_base_100.\.csv]  # Filters input files
  #input_mask_regex: [dsm2_base_100\.csv, dsm2_base_100.\.csv]  # Filters input files  
  input_mask_regex: [dsm2_base_100.\.csv,dsm2_base_historical\.csv]  # Filters input files    
  output_prefix: "{output_dir}/dsm2_base_gru2"  # Prefix for saved results
  save_model_fname: "{output_dir}/dsm2_base_gru2"  # Final model filename
  load_model_fname: None  # First modelâ€”no prior model to load
  pool_size: 12  # Number of parallel training threads
  pool_aggregation: True  # Use pool aggregation for large case counts
  target_fold_length: 180d  # Cross-validation time fold length
  init_train_rate: 0.008  # Initial learning rate for warm-up
  init_epochs: 10  # Short warm-up phase 
  main_train_rate: 0.001  # Standard learning rate for main training phase
  main_epochs: 100  # Number of epochs for main training phase was 100
  builder_args:  
    transfer_type: None  # No transfer learning in the base DSM2 training
    ann_output_name: dsm2_base  # Name of output (applies if there is one output head, otherwise use ann_output_name_map)
    rnn_type: LSTM         # or GRU
    rnn_units: [32, 16]    # List defines layers: 2 layers with 32 and 16 units respectively
# --------------------------------------------------------------
- name: dsm2.schism  # Step 2: Transfer DSM2 Knowledge to SCHISM (3D Model)
  input_prefix: schism_base  # Points to SCHISM dataset (aligned with DSM2)
  input_mask_regex: None  # No need for filtering
  output_prefix: "{output_dir}/dsm2.schism_base_gru2"  # New model output filename
  save_model_fname: "{output_dir}/dsm2.schism_base_gru2"  # File to save the transfer-learned model
  load_model_fname: "{output_dir}/dsm2_base_gru2"  # Start from DSM2 model
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.003  # Medium learning rate, helpful but tune down if you see catostrophic forgetting (see docs)
  init_epochs: 11  # Transfer learning often requires fewer epochs was 10 then 35
  main_train_rate: 0.001  # Only a single training phase
  main_epochs: 61  # No separate main training phase was 25
  builder_args:
    transfer_type: direct  # Standard transfer learning
    save_modified_orig_model_fname: None  # No need to modify the DSM2 model
    ann_output_name: schsim_base
    rnn_type: LSTM         # or GRU
    rnn_units: [32, 16]    # List defines layers: 2 layers with 32 and 16 units respectively

# --------------------------------------------------------------
- name: base.suisun  # Step 3: Transfer SCHISM (Base Scenario) to SCHISM (Suisun Scenario)
  input_prefix: schism_suisun  # Dataset for the Suisun scenario
  input_mask_regex: None
  output_prefix: "{output_dir}/schism_base.suisun_gru2"  # New model name
  save_model_fname: "{output_dir}/schism_base.suisun_gru2"  # Final model filename
  load_model_fname: "{output_dir}/dsm2.schism_base_gru2"  # Start from SCHISM (Base)
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.001  # Even lower learning rate for fine-tuning
  init_epochs: 12 # Short fine-tuning phase
  main_train_rate: 0.0005  # Main learning rate for adaptation
  main_epochs: 62  # Extended training for scenario adaptation
  builder_args:
    source_data_prefix: schism_base
    source_input_mask_regex: [schism_base_2021.csv ]
    transfer_type: contrastive  # Use contrastive learning to retain differences
    contrast_weight: 0.5 
    save_modified_orig_model_fname: schism_base-suisun_gru2  # Keep modified version of SCHISM (Base)
    rnn_type: LSTM         # or GRU
    rnn_units: [32, 16]    # List defines layers: 2 layers with 32 and 16 units respectively

# --------------------------------------------------------------
- name: base.slr  # Step 3: Transfer SCHISM (Base Scenario) to SCHISM (SLR Scenario)
  input_prefix: schism_slr_base  # Dataset for the SLR scenario
  input_mask_regex: None
  output_prefix: "{output_dir}/schism_base.slr_gru2"  # New model name
  save_model_fname: "{output_dir}/schism_base.slr_gru2"  # Final model filename
  load_model_fname: "{output_dir}/dsm2.schism_base_gru2"  # Start from SCHISM (Base)
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.001  # Even lower learning rate for fine-tuning
  init_epochs: 12 # Short fine-tuning phase
  main_train_rate: 0.0005  # Main learning rate for adaptation
  main_epochs: 62  # Extended training for scenario adaptation
  builder_args:
    source_data_prefix: schism_base
    source_input_mask_regex: [schism_base_2021.csv ]
    transfer_type: contrastive  # Use contrastive learning to retain differences
    contrast_weight: 0.5
    save_modified_orig_model_fname: schism_base-slr_gru2  # Keep modified version of SCHISM (Base)
    rnn_type: LSTM         # or GRU
    rnn_units: [32, 16]    # List defines layers: 2 layers with 32 and 16 units respectively

# --------------------------------------------------------------
- name: base.ft  # Step 3: Transfer SCHISM (Base Scenario) to SCHISM (Franks Scenario)
  input_prefix: schism_franks  # Dataset for the Suisun scenario
  input_mask_regex: None
  output_prefix: "{output_dir}/schism_base.ft_gru2"  # New model name
  save_model_fname: "{output_dir}/schism_base.ft_gru2"  # Final model filename
  load_model_fname: "{output_dir}/dsm2.schism_base_gru2"  # Start from SCHISM (Base)
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.001  # Even lower learning rate for fine-tuning
  init_epochs: 12 # Short fine-tuning phase
  main_train_rate: 0.0005  # Main learning rate for adaptation
  main_epochs: 62  # Extended training for scenario adaptation
  builder_args:
    source_data_prefix: schism_base
    source_input_mask_regex: [schism_base_2021.csv ]
    transfer_type: contrastive  # Use contrastive learning to retain differences
    contrast_weight: 0.5
    save_modified_orig_model_fname: schism_base-ft_gru2  # Keep modified version of SCHISM (Base)
    rnn_type: LSTM         # or GRU
    rnn_units: [32, 16]    # List defines layers: 2 layers with 32 and 16 units respectively

# --------------------------------------------------------------
- name: base.cache  # Step 3: Transfer SCHISM (Base Scenario) to SCHISM (Cache Scenario)
  input_prefix: schism_cache  # Dataset for the Suisun scenario
  input_mask_regex: None
  output_prefix: "{output_dir}/schism_base.cache_gru2"  # New model name
  save_model_fname: "{output_dir}/schism_base.cache_gru2"  # Final model filename
  load_model_fname: "{output_dir}/dsm2.schism_base_gru2"  # Start from SCHISM (Base)
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.001  # Even lower learning rate for fine-tuning
  init_epochs: 12 # Short fine-tuning phase
  main_train_rate: 0.0005  # Main learning rate for adaptation
  main_epochs: 62  # Extended training for scenario adaptation
  builder_args:
    source_data_prefix: schism_base
    source_input_mask_regex: [ schism_base_2021.csv ]
    transfer_type: contrastive  # Use contrastive learning to retain differences
    contrast_weight: 0.5
    save_modified_orig_model_fname: schism_base-cache_gru2  # Keep modified version of SCHISM (Base)
    rnn_type: LSTM         # or GRU
    rnn_units: [ 32, 16 ]    # List defines layers: 2 layers with 32 and 16 units respectively

# --------------------------------------------------------------
- name: dsm2.rma  # Step 2: Transfer DSM2 Knowledge to RMA
  input_prefix: rma_base  # Points to RMA dataset (aligned with DSM2)
  input_mask_regex: None  # No need for filtering
  output_prefix: "{output_dir}/dsm2.rma_base_gru2"  # New model output filename
  save_model_fname: "{output_dir}/dsm2.rma_base_gru2"  # File to save the transfer-learned model
  load_model_fname: "{output_dir}/dsm2_base_gru2"  # Start from DSM2 model
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.003  # Medium learning rate, helpful but tune down if you see catostrophic forgetting (see docs)
  init_epochs: 11  # Transfer learning often requires fewer epochs was 10 then 35
  main_train_rate: 0.001  # Only a single training phase
  main_epochs: 61  # No separate main training phase was 25
  builder_args:
    transfer_type: direct  # Standard transfer learning
    save_modified_orig_model_fname: None  # No need to modify the DSM2 model
    ann_output_name: rma_base
    rnn_type: LSTM         # or GRU
    rnn_units: [32, 16]    # List defines layers: 2 layers with 32 and 16 units respectively

# --------------------------------------------------------------
- name: rma_base.suisun  # Step 3: Transfer RMA (Base Scenario) to RMA (Suisun Scenario)
  input_prefix: rma_suisun  # Dataset for the Suisun scenario
  input_mask_regex: None
  output_prefix: "{output_dir}/rma_base.suisun_gru2"  # New model name
  save_model_fname: "{output_dir}/rma_base.suisun_gru2"  # Final model filename
  load_model_fname: "{output_dir}/dsm2.rma_base_gru2"  # Start from RMA (Base)
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.001  # Even lower learning rate for fine-tuning
  init_epochs: 12 # Short fine-tuning phase
  main_train_rate: 0.0005  # Main learning rate for adaptation
  main_epochs: 62  # Extended training for scenario adaptation
  builder_args:
    source_data_prefix: rma_base
    source_input_mask_regex: [rma_base_*.csv ]
    transfer_type: contrastive  # Use contrastive learning to retain differences
    contrast_weight: 0.5
    save_modified_orig_model_fname: rma_base-suisun_gru2  # Keep modified version of RMA (Base)
    rnn_type: LSTM         # or GRU
    rnn_units: [32, 16]    # List defines layers: 2 layers with 32 and 16 units respectively

# --------------------------------------------------------------
- name: rma_base.ft  # Step 3: Transfer RMA (Base Scenario) to RMA (Franks Tract Scenario)
  input_prefix: rma_ft  # Dataset for the Franks scenario
  input_mask_regex: None
  output_prefix: "{output_dir}/rma_base.ft_gru2"  # New model name
  save_model_fname: "{output_dir}/rma_base.ft_gru2"  # Final model filename
  load_model_fname: "{output_dir}/dsm2.rma_base_gru2"  # Start from RMA (Base)
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.001  # Even lower learning rate for fine-tuning
  init_epochs: 12 # Short fine-tuning phase
  main_train_rate: 0.0005  # Main learning rate for adaptation
  main_epochs: 62  # Extended training for scenario adaptation
  builder_args:
    source_data_prefix: rma_base
    source_input_mask_regex: [rma_base_*.csv ]
    transfer_type: contrastive  # Use contrastive learning to retain differences
    contrast_weight: 0.5
    save_modified_orig_model_fname: rma_base-ft_gru2  # Keep modified version of RMA (Base)
    rnn_type: LSTM         # or GRU
    rnn_units: [32, 16]    # List defines layers: 2 layers with 32 and 16 units respectively

# --------------------------------------------------------------
- name: rma_base.cache  # Step 3: Transfer RMA (Base Scenario) to RMA (Cache Scenario)
  input_prefix: rma_cache  # Dataset for the Cache scenario
  input_mask_regex: None
  output_prefix: "{output_dir}/rma_base.cache_gru2"  # New model name
  save_model_fname: "{output_dir}/rma_base.cache_gru2"  # Final model filename
  load_model_fname: "{output_dir}/dsm2.rma_base_gru2"  # Start from RMA (Base)
  pool_size: 12
  pool_aggregation: False
  target_fold_length: 180d
  init_train_rate: 0.001  # Even lower learning rate for fine-tuning
  init_epochs: 12 # Short fine-tuning phase
  main_train_rate: 0.0005  # Main learning rate for adaptation
  main_epochs: 62  # Extended training for scenario adaptation
  builder_args:
    source_data_prefix: rma_base
    source_input_mask_regex: [rma_base_*.csv ]
    transfer_type: contrastive  # Use contrastive learning to retain differences
    contrast_weight: 0.5
    save_modified_orig_model_fname: rma_base-cache_gru2  # Keep modified version of RMA (Base)
    rnn_type: LSTM         # or GRU
    rnn_units: [32, 16]    # List defines layers: 2 layers with 32 and 16 units respectively